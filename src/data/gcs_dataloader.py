"""
GCS ê¸°ë°˜ ë°ì´í„°ë¡œë” with ë©€í‹°í”„ë¡œì„¸ìŠ¤ ìºì‹± (112 vCPU)
- GCSì—ì„œ PT íŒŒì¼ ìë™ ìˆœíšŒ
- Parquet ë©”íƒ€ë°ì´í„° ìºì‹±
- 112 workersë¡œ ë³‘ë ¬ ë¡œë”© ë° prefetch
"""

import os
import gc
import queue
import threading
import tempfile
import logging
from pathlib import Path
from typing import List, Optional, Tuple, Dict
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
import torch
import pyarrow.parquet as pq
import jax
import jax.numpy as jnp

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================
# GCS íŒŒì¼ í•¸ë“¤ëŸ¬
# ============================================
class GCSFileHandler:
    """GCS íŒŒì¼ ê´€ë¦¬"""
    
    def __init__(self, gcs_bucket: str = "gs://rdy-tpu-data-2025/coyo11m-256px-ccrop-latent/"):
        self.gcs_bucket = gcs_bucket
        self.latent_path = f"{gcs_bucket}latents-3crop/"
        self.metadata_path = gcs_bucket
        
        # gsutil ë˜ëŠ” gcloud storage ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        self._check_gcs_availability()
    
    def _check_gcs_availability(self):
        """GCS ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import subprocess
            result = subprocess.run(
                ["gcloud", "storage", "ls", self.gcs_bucket],
                capture_output=True, text=True, timeout=10
            )
            self.gcs_available = result.returncode == 0
            if self.gcs_available:
                logger.info("âœ“ GCS access verified")
            else:
                logger.warning(f"âœ— GCS access failed: {result.stderr}")
        except Exception as e:
            self.gcs_available = False
            logger.warning(f"GCS check failed: {e}")
    
    def list_pt_files(self) -> List[str]:
        """GCSì—ì„œ PT íŒŒì¼ ëª©ë¡ ì¡°íšŒ (ì •ë ¬ë¨)"""
        if not self.gcs_available:
            logger.warning("GCS not available, returning empty list")
            return []
        
        try:
            import subprocess
            result = subprocess.run(
                ["gcloud", "storage", "ls", f"{self.latent_path}*.pt"],
                capture_output=True, text=True, timeout=30
            )
            
            if result.returncode != 0:
                logger.error(f"Failed to list files: {result.stderr}")
                return []
            
            files = [line.strip() for line in result.stdout.strip().split("\n") if line.strip()]
            # íŒŒì¼ëª…ìœ¼ë¡œ ì •ë ¬ (000000-000009.pt, 000010-000019.pt, ...)
            files.sort()
            logger.info(f"Found {len(files)} PT files")
            return files
        except Exception as e:
            logger.error(f"Error listing PT files: {e}")
            return []
    
    def download_file(self, gcs_path: str, local_path: str) -> bool:
        """GCS íŒŒì¼ì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œ"""
        if not self.gcs_available:
            logger.warning(f"GCS not available, cannot download {gcs_path}")
            return False
        
        try:
            import subprocess
            Path(local_path).parent.mkdir(parents=True, exist_ok=True)
            
            result = subprocess.run(
                ["gcloud", "storage", "cp", gcs_path, local_path],
                capture_output=True, text=True, timeout=300  # 5ë¶„
            )
            
            if result.returncode == 0:
                logger.info(f"âœ“ Downloaded: {gcs_path}")
                return True
            else:
                logger.error(f"Download failed: {result.stderr}")
                return False
        except Exception as e:
            logger.error(f"Error downloading {gcs_path}: {e}")
            return False


# ============================================
# íŒŒì¿ ì—£ ë©”íƒ€ë°ì´í„° ìºì‹œ
# ============================================
@dataclass
class ParquetCache:
    """Parquet ë©”íƒ€ë°ì´í„° ìºì‹±"""
    key_to_caption: Dict[int, str]
    key_to_url: Dict[int, str]
    key_to_caption_llava: Dict[int, str]
    all_keys: set
    
    @classmethod
    def load_from_parquet(cls, parquet_path: str) -> 'ParquetCache':
        """Parquet íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        logger.info(f"Loading parquet metadata: {parquet_path}")
        
        try:
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œ
            table = pq.read_table(
                parquet_path,
                columns=['key', 'caption_llava', 'url']
            )
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ dictë¡œ ë³€í™˜
            key_to_caption_llava = {}
            key_to_url = {}
            
            for batch in table.to_batches():
                keys = batch['key'].to_pylist()
                captions = batch['caption_llava'].to_pylist()
                urls = batch['url'].to_pylist()
                
                for k, c, u in zip(keys, captions, urls):
                    key_to_caption_llava[k] = c or ""
                    key_to_url[k] = u or ""
            
            logger.info(f"âœ“ Loaded {len(key_to_caption_llava)} metadata entries")
            
            return cls(
                key_to_caption=key_to_caption_llava,  # caption_llavaë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
                key_to_url=key_to_url,
                key_to_caption_llava=key_to_caption_llava,
                all_keys=set(key_to_caption_llava.keys())
            )
        except Exception as e:
            logger.error(f"Failed to load parquet: {e}")
            raise


# ============================================
# GCS ê¸°ë°˜ ë°ì´í„°ë¡œë”
# ============================================
class GCSCoyo11mDataLoader:
    """GCSì˜ PT íŒŒì¼ê³¼ Parquet ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë°ì´í„°ë¡œë”
    
    PT íŒŒì¼ì„ ë™ì ìœ¼ë¡œ ë¡œë“œ/ì–¸ë¡œë“œí•˜ì—¬ ë””ìŠ¤í¬ ê³µê°„ ìµœì í™”
    """
    
    def __init__(
        self,
        batch_size: int,
        parquet_cache: ParquetCache,
        embedding_provider=None,
        gcs_bucket: str = "gs://rdy-tpu-data-2025/coyo11m-256px-ccrop-latent/",
        cache_dir: Optional[str] = None,
        num_samples: Optional[int] = None,
        max_cache_files: int = 3  # ìµœëŒ€ 3ê°œ PT íŒŒì¼ê¹Œì§€ë§Œ ë©”ëª¨ë¦¬ ìœ ì§€
    ):
        self.batch_size = batch_size
        self.embedding_provider = embedding_provider
        self.parquet_cache = parquet_cache
        self.gcs_handler = GCSFileHandler(gcs_bucket)
        self.max_cache_files = max_cache_files
        
        # ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬
        if cache_dir is None:
            cache_dir = tempfile.gettempdir()
        self.cache_dir = Path(cache_dir) / "gcs_pt_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.current_pt_file = None
        self.available_indices = []
        self.pt_keys = None
        self.latents_torch = None
        
        # ìºì‹œ ì¶”ì 
        self.loaded_files = {}  # filename -> load_time
        self.num_samples = num_samples
        
        logger.info(f"Cache dir: {self.cache_dir} (max {max_cache_files} files at once)")
    
    def _cleanup_old_files(self):
        """ì˜¤ë˜ëœ PT íŒŒì¼ ì‚­ì œí•˜ì—¬ ë””ìŠ¤í¬ ê³µê°„ í™•ë³´ (max_cache_files ìœ ì§€)"""
        if len(self.loaded_files) >= self.max_cache_files:
            # ê°€ì¥ ì˜¤ë˜ ë¡œë“œëœ íŒŒì¼ ì‚­ì œ
            oldest_file = min(self.loaded_files.items(), key=lambda x: x[1])[0]
            old_path = self.cache_dir / oldest_file
            
            if old_path.exists():
                try:
                    file_size_mb = old_path.stat().st_size / (1024**2)
                    old_path.unlink()
                    del self.loaded_files[oldest_file]
                    logger.info(f"  ğŸ—‘ï¸ Cleaned cache: removed {oldest_file} ({file_size_mb:.1f}MB)")
                except Exception as e:
                    logger.warning(f"Failed to delete {oldest_file}: {e}")
    
    def load_pt_file(self, gcs_pt_path: str) -> bool:
        """GCSì—ì„œ PT íŒŒì¼ ë¡œë“œ (ìë™ ìºì‹œ ê´€ë¦¬)"""
        try:
            # íŒŒì¼ëª… ì¶”ì¶œ
            pt_filename = gcs_pt_path.split("/")[-1]
            local_pt_path = self.cache_dir / pt_filename
            
            # ì´ë¯¸ ë¡œì»¬ì— ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
            if not local_pt_path.exists():
                logger.info(f"Downloading PT file: {pt_filename}")
                if not self.gcs_handler.download_file(gcs_pt_path, str(local_pt_path)):
                    return False
            
            # ìºì‹œ ì •ë¦¬ (ìµœëŒ€ Nê°œ íŒŒì¼ë§Œ ìœ ì§€)
            self._cleanup_old_files()
            
            # PT íŒŒì¼ ë¡œë“œ
            logger.info(f"Loading PT file: {pt_filename}")
            pt_data = torch.load(str(local_pt_path), map_location="cpu")
            
            self.pt_keys = pt_data['keys'].numpy()
            self.latents_torch = pt_data['latents']
            self.current_pt_file = pt_filename
            
            # ë¡œë“œ ì‹œê°„ ê¸°ë¡
            import time
            self.loaded_files[pt_filename] = time.time()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ ì°¾ê¸° (PTì™€ Parquet ëª¨ë‘ì— ìˆëŠ” ê²ƒ)
            self._find_available_indices()
            
            file_size_mb = local_pt_path.stat().st_size / (1024**2)
            logger.info(f"âœ“ Loaded {pt_filename} ({file_size_mb:.1f}MB) with {len(self.available_indices)} samples")
            return True
        except Exception as e:
            logger.error(f"Error loading PT file {gcs_pt_path}: {e}")
            return False
    
    def _find_available_indices(self):
        """PTì™€ Parquet ëª¨ë‘ì— ìˆëŠ” ìƒ˜í”Œ ì¸ë±ìŠ¤ ì°¾ê¸°"""
        self.available_indices = []
        
        limit = self.num_samples if self.num_samples else len(self.pt_keys)
        for idx, key in enumerate(self.pt_keys[:limit]):
            if key in self.parquet_cache.all_keys:
                self.available_indices.append(idx)
    
    def get_batch(self, batch_idx: int, rng_key) -> Tuple[jnp.ndarray, jnp.ndarray]:
        """ë°°ì¹˜ ë°ì´í„° ë°˜í™˜ (ëœë¤ ìƒ˜í”Œë§)"""
        if not self.available_indices:
            raise ValueError("No available samples in current PT file")
        
        # ëœë¤ ì¸ë±ìŠ¤ ì„ íƒ
        indices = jax.random.randint(rng_key, (self.batch_size,), 0, len(self.available_indices))
        indices_np = np.array(indices)
        selected_indices = [self.available_indices[i] for i in indices_np]
        
        # Latent ì¶”ì¶œ: (B, 4, 32, 32) NCHW â†’ (B, 32, 32, 4) NHWC
        latents_subset = self.latents_torch[selected_indices]  # (B, 4, 32, 32)
        latents_np = latents_subset.float().numpy().astype(np.float32)
        batch_latents = jnp.array(np.transpose(latents_np, (0, 2, 3, 1)))  # (B, 32, 32, 4)
        
        # Caption ì¶”ì¶œ
        batch_captions = []
        for idx in selected_indices:
            key = int(self.pt_keys[idx])
            caption = self.parquet_cache.key_to_caption.get(key, "")
            batch_captions.append(caption)
        
        # ì„ë² ë”© ê³„ì‚°
        batch_embeddings = self.embedding_provider.batch_encode(
            batch_captions, batch_size=512, normalize=True
        )
        
        return batch_latents, batch_embeddings


# ============================================
# ë³‘ë ¬ ìºì‹± ë§¤ë‹ˆì € (112 workers)
# ============================================
class ParallelCacheManager:
    """ë©€í‹°í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ìºì‹± ë§¤ë‹ˆì €"""
    
    def __init__(self, num_workers: int = 112, cache_dir: Optional[str] = None):
        self.num_workers = num_workers
        self.cache_dir = Path(cache_dir or tempfile.gettempdir()) / "gcs_pt_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.executor = ThreadPoolExecutor(max_workers=num_workers)
        self.download_tasks = {}
        self.cached_files = set()
        
        logger.info(f"Initialized ParallelCacheManager with {num_workers} workers")
    
    def prefetch_pt_files(self, pt_files: List[str], gcs_handler: GCSFileHandler):
        """PT íŒŒì¼ ëª©ë¡ì„ ë³‘ë ¬ë¡œ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ"""
        logger.info(f"Starting parallel prefetch of {len(pt_files)} PT files")
        
        for gcs_path in pt_files:
            filename = gcs_path.split("/")[-1]
            local_path = self.cache_dir / filename
            
            # ì´ë¯¸ ìºì‹œë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if local_path.exists():
                self.cached_files.add(filename)
                continue
            
            # ë‹¤ìš´ë¡œë“œ íƒœìŠ¤í¬ ì œì¶œ
            future = self.executor.submit(
                gcs_handler.download_file, gcs_path, str(local_path)
            )
            self.download_tasks[filename] = future
        
        logger.info(f"Submitted {len(self.download_tasks)} download tasks")
    
    def wait_for_file(self, filename: str, timeout: int = 600) -> bool:
        """íŠ¹ì • íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ timeoutì´ˆ)"""
        if filename in self.cached_files:
            return True
        
        if filename not in self.download_tasks:
            logger.warning(f"File {filename} not in download queue")
            return False
        
        try:
            future = self.download_tasks[filename]
            result = future.result(timeout=timeout)
            if result:
                self.cached_files.add(filename)
            return result
        except Exception as e:
            logger.error(f"Error waiting for {filename}: {e}")
            return False
    
    def shutdown(self):
        """ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.executor.shutdown(wait=True)
        logger.info("ParallelCacheManager shutdown complete")


# ============================================
# GCS Prefetch Data Loader (112 workers)
# ============================================
class GCSPrefetchDataLoader:
    """GCS ê¸°ë°˜ Prefetch ë°ì´í„°ë¡œë”© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(
        self,
        data_loader: GCSCoyo11mDataLoader,
        pt_files: List[str],
        steps_per_epoch: int,
        num_workers: int = 112,
        cache_manager: Optional[ParallelCacheManager] = None
    ):
        self.data_loader = data_loader
        self.pt_files = pt_files
        self.steps_per_epoch = steps_per_epoch
        self.num_workers = num_workers
        self.cache_manager = cache_manager or ParallelCacheManager(num_workers)
        
        self.prefetch_queue = queue.Queue(maxsize=num_workers * 2)
        self.stop_event = threading.Event()
        
        self.executor = ThreadPoolExecutor(max_workers=num_workers)
        self.worker_threads = []
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ PT íŒŒì¼ prefetch ì‹œì‘
        self._start_prefetch()
    
    def _start_prefetch(self):
        """Prefetch worker ì‹œì‘"""
        for worker_id in range(self.num_workers):
            thread = threading.Thread(
                target=self._prefetch_worker,
                args=(worker_id,),
                daemon=True
            )
            thread.start()
            self.worker_threads.append(thread)
    
    def _prefetch_worker(self, worker_id: int):
        """ê° workerê°€ ë‹´ë‹¹ ë°°ì¹˜ë¥¼ ë¯¸ë¦¬ ë¡œë“œ"""
        rng_key = jax.random.PRNGKey(worker_id)
        current_pt_idx = 0
        current_batch_in_pt = 0
        
        while current_pt_idx < len(self.pt_files):
            if self.stop_event.is_set():
                break
            
            try:
                # PT íŒŒì¼ ë¡œë“œ
                pt_file = self.pt_files[current_pt_idx]
                pt_filename = pt_file.split("/")[-1]
                
                # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
                if not self.cache_manager.wait_for_file(pt_filename):
                    logger.warning(f"Failed to load {pt_filename}, skipping")
                    current_pt_idx += 1
                    current_batch_in_pt = 0
                    continue
                
                # PT íŒŒì¼ ë¡œë“œ
                if current_batch_in_pt == 0:
                    if not self.data_loader.load_pt_file(pt_file):
                        current_pt_idx += 1
                        continue
                
                # ë°°ì¹˜ ìƒì„±
                if current_batch_in_pt < self.steps_per_epoch:
                    rng_key, subkey = jax.random.split(rng_key)
                    batch = self.data_loader.get_batch(current_batch_in_pt, subkey)
                    
                    global_batch_idx = current_pt_idx * self.steps_per_epoch + current_batch_in_pt
                    self.prefetch_queue.put((global_batch_idx, batch), timeout=30)
                    
                    current_batch_in_pt += self.num_workers
                else:
                    current_pt_idx += 1
                    current_batch_in_pt = 0
            
            except Exception as e:
                logger.error(f"Prefetch error (worker {worker_id}): {e}")
                break
        
        self.prefetch_queue.put(None)
    
    def get_batches(self):
        """Prefetchëœ ë°°ì¹˜ ë°˜í™˜"""
        batches_dict = {}
        next_batch_idx = 0
        none_count = 0
        
        total_batches = len(self.pt_files) * self.steps_per_epoch
        
        while next_batch_idx < total_batches:
            try:
                item = self.prefetch_queue.get(timeout=60)
                
                if item is None:
                    none_count += 1
                    if none_count >= self.num_workers:
                        break
                    continue
                
                batch_idx, batch = item
                batches_dict[batch_idx] = batch
                
                # ìˆœì°¨ì ìœ¼ë¡œ ë°°ì¹˜ ë°˜í™˜
                while next_batch_idx in batches_dict:
                    yield batches_dict.pop(next_batch_idx)
                    next_batch_idx += 1
            
            except queue.Empty:
                logger.warning(f"Prefetch timeout at batch {next_batch_idx}")
                break
    
    def stop(self):
        """Prefetch ì¤‘ì§€"""
        self.stop_event.set()
        self.executor.shutdown(wait=True)
        for thread in self.worker_threads:
            thread.join(timeout=5)
        self.cache_manager.shutdown()


# ============================================
# ì„¸ì…˜ ê¸°ë°˜ ë°ì´í„°ë¡œë” (epochë§ˆë‹¤ PT íŒŒì¼ ìˆœíšŒ)
# ============================================
class GCSDataLoaderSession:
    """GCS ë°ì´í„°ë¥¼ ì´ìš©í•œ ì„¸ì…˜ ê¸°ë°˜ ë¡œë”
    
    ë””ìŠ¤í¬ ê³µê°„ ìµœì í™”ë¥¼ ìœ„í•´ ìë™ìœ¼ë¡œ ì˜¤ë˜ëœ PT íŒŒì¼ ì‚­ì œ
    """
    
    def __init__(
        self,
        batch_size: int,
        parquet_path: str,
        embedding_provider=None,
        gcs_bucket: str = "gs://rdy-tpu-data-2025/coyo11m-256px-ccrop-latent/",
        cache_dir: Optional[str] = None,
        num_workers: int = 112,
        prefetch_ahead: int = 2,  # ëª‡ ê°œì˜ PT íŒŒì¼ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí• ì§€
        max_cache_files: int = 3  # ìµœëŒ€ ëª‡ ê°œì˜ PT íŒŒì¼ì„ ë™ì‹œì— ë³´ê´€í• ì§€ (ë””ìŠ¤í¬ ì ˆì•½)
    ):
        self.batch_size = batch_size
        self.embedding_provider = embedding_provider
        self.gcs_bucket = gcs_bucket
        self.cache_dir = cache_dir
        self.num_workers = num_workers
        self.prefetch_ahead = prefetch_ahead
        self.max_cache_files = max_cache_files
        
        # Parquet ë©”íƒ€ë°ì´í„° ìºì‹±
        logger.info("Loading parquet metadata cache...")
        self.parquet_cache = ParquetCache.load_from_parquet(parquet_path)
        
        # GCS í•¸ë“¤ëŸ¬ ë° ìºì‹œ ë§¤ë‹ˆì €
        self.gcs_handler = GCSFileHandler(gcs_bucket)
        self.cache_manager = ParallelCacheManager(num_workers, cache_dir)
        
        # PT íŒŒì¼ ëª©ë¡
        self.pt_files = self.gcs_handler.list_pt_files()
        if not self.pt_files:
            raise ValueError("No PT files found in GCS bucket")
        
        logger.info(f"âœ“ Session initialized")
        logger.info(f"  Total PT files: {len(self.pt_files)}")
        logger.info(f"  Max cache files: {max_cache_files} (disk space optimized)")
        logger.info(f"  Expected disk usage: ~{max_cache_files * 100}MB")
        
        # ì²« ë°°ì¹˜ì˜ PT íŒŒì¼ë“¤ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ
        self._prefetch_initial_files()
    
    def _prefetch_initial_files(self):
        """ì²˜ìŒ prefetch_aheadê°œì˜ PT íŒŒì¼ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ"""
        files_to_prefetch = self.pt_files[:min(self.prefetch_ahead, len(self.pt_files))]
        self.cache_manager.prefetch_pt_files(files_to_prefetch, self.gcs_handler)
    
    def get_epoch_loader(self, epoch: int, steps_per_epoch: int) -> GCSPrefetchDataLoader:
        """ì—í¬í¬ìš© prefetch ë¡œë” ìƒì„±"""
        logger.info(f"\n{'='*70}")
        logger.info(f"Creating epoch loader for epoch {epoch}")
        logger.info(f"{'='*70}")
        
        # í˜„ì¬ ì—í¬í¬ì— í•„ìš”í•œ PT íŒŒì¼ë“¤ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ
        next_prefetch_start = min(self.prefetch_ahead, len(self.pt_files))
        next_prefetch_end = min(next_prefetch_start + self.prefetch_ahead, len(self.pt_files))
        
        if next_prefetch_start < len(self.pt_files):
            files_to_prefetch = self.pt_files[next_prefetch_start:next_prefetch_end]
            self.cache_manager.prefetch_pt_files(files_to_prefetch, self.gcs_handler)
        
        # ë°ì´í„°ë¡œë” ìƒì„± (ìºì‹œ íŒŒì¼ ê°œìˆ˜ ì œí•œ)
        data_loader = GCSCoyo11mDataLoader(
            batch_size=self.batch_size,
            parquet_cache=self.parquet_cache,
            embedding_provider=self.embedding_provider,
            gcs_bucket=self.gcs_bucket,
            cache_dir=self.cache_dir,
            max_cache_files=self.max_cache_files  # ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½
        )
        
        # Prefetch ë¡œë” ìƒì„±
        return GCSPrefetchDataLoader(
            data_loader=data_loader,
            pt_files=self.pt_files,
            steps_per_epoch=steps_per_epoch,
            num_workers=self.num_workers,
            cache_manager=self.cache_manager
        )
    
    def shutdown(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        self.cache_manager.shutdown()
        logger.info("GCS DataLoader Session shutdown complete")
