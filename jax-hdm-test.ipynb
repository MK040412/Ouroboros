{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f04e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current backend: cpu\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "\n",
    "print(\"Current backend:\", jax.default_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "059b6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0]\n",
      "JAX version: 0.5.3\n",
      "Devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax, sys\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "print(\"Devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc7e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "RMSNorm = nnx.RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b0993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4136842   1.7644585  -0.37775773 -0.06849329  0.15341455 -0.8469071 ]\n",
      " [-0.37608212  0.37538347  0.5044429  -0.7214626   1.6549253  -1.484553  ]\n",
      " [ 0.37910467  0.16683145  1.3502184   1.5969633   1.0262417   0.6339453 ]\n",
      " [ 0.01842601 -1.4296368  -1.387021    1.2891457   0.03520264  0.607316  ]\n",
      " [ 0.17680073  0.38078466  1.6741968   0.92929405 -1.0779979  -0.9975626 ]]\n"
     ]
    }
   ],
   "source": [
    "key = nnx.Rngs(42)\n",
    "hidden_size = (5, 6)\n",
    "x = jax.random.normal(jax.random.key(0), hidden_size)\n",
    "norm = RMSNorm(num_features=6,rngs=key)\n",
    "output = norm(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1da1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.5698478   0.777241   -0.01974281 ... -0.6180885  -0.46066764\n",
      "    0.6937558 ]\n",
      "  [-0.26934257 -0.50588846  0.4076102  ... -0.9948675   0.10751119\n",
      "   -0.94601625]\n",
      "  [-0.31217194 -0.9516002   0.5457004  ...  0.03558588 -0.37972584\n",
      "    0.41939488]\n",
      "  ...\n",
      "  [-0.83006567  0.40642098 -1.0011536  ...  0.6166688  -1.0053461\n",
      "   -0.4291304 ]\n",
      "  [-0.02426559  0.67361283 -0.258857   ...  0.26960015  0.12398798\n",
      "    0.44029686]\n",
      "  [-0.6357401  -0.05438824 -0.837344   ... -0.25044262 -0.0588683\n",
      "    0.34996915]]\n",
      "\n",
      " [[-0.6730153   0.78862244  0.09795742 ... -0.8814044   0.9523628\n",
      "   -0.2764015 ]\n",
      "  [ 0.4803267   0.35329565 -0.43849137 ...  0.36068055 -0.98329335\n",
      "   -0.09067731]\n",
      "  [-0.79592603 -0.44201332 -0.21084595 ... -0.60548025  0.7878249\n",
      "   -1.5968543 ]\n",
      "  ...\n",
      "  [ 0.27719223  0.16724351 -0.27431744 ...  0.6415455   0.33255875\n",
      "    0.12926781]\n",
      "  [ 0.05443227  0.44054908  0.20841222 ...  0.6051239  -0.3080075\n",
      "   -1.076773  ]\n",
      "  [ 0.1794864   0.4517041   0.14160526 ... -0.18415213  0.5466403\n",
      "   -0.1568167 ]]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "class SwiGLU(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: int = None,\n",
    "        out_features: int = None,\n",
    "        *,\n",
    "        use_bias: bool = True,\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features or in_features\n",
    "        self.out_features = out_features or in_features\n",
    "        self.w12 = nnx.Linear(self.in_features, 2*self.hidden_features, use_bias=use_bias, rngs=rngs)\n",
    "        self.w3 = nnx.Linear(self.hidden_features, self.out_features, use_bias=use_bias, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        w12 = self.w12(x)\n",
    "        x1, x2 = jnp.split(w12, 2, axis=-1)\n",
    "        return self.w3(nnx.silu(x1) * x2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = jax.random.normal(jax.random.PRNGKey(42), (2, 16, 128))\n",
    "    model = SwiGLU(128, 256, 128, rngs=nnx.Rngs(params=42))\n",
    "    output = model(x)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6583a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape = (2, 16), gate.shape = (2, 16)\n",
      "output.shape = (2, 16), gate.shape = (2, 16)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from flax.nnx import rnglib\n",
    "from flax.nnx import RMSNorm\n",
    "\n",
    "class AdaLN(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        y_dim: int,\n",
    "        *,\n",
    "        gate: bool = True,\n",
    "        norm_layer: nnx.Module = RMSNorm,\n",
    "        shared: bool = False,\n",
    "        rngs: rnglib.Rngs,\n",
    "    ):\n",
    "        self.norm = norm_layer(num_features=dim, rngs=rngs)\n",
    "        self.gate = gate\n",
    "        if shared:\n",
    "            self.adaln = None\n",
    "        else:\n",
    "            self.adaln = nnx.Linear(\n",
    "                y_dim,\n",
    "                dim * (2 + int(self.gate)),\n",
    "                use_bias = True,\n",
    "                bias_init = nnx.initializers.zeros_init(),\n",
    "                kernel_init = nnx.initializers.zeros_init(),\n",
    "                rngs = rngs,\n",
    "            )\n",
    "\n",
    "    def __call__(self, x, y, shared_adaln=None):\n",
    "        if shared_adaln is None:\n",
    "            out = self.adaln(y)\n",
    "            scale, shift, *gate = jnp.split(out, 2 + int(self.gate), axis=-1)\n",
    "        else:\n",
    "            scale, shift, *gate = shared_adaln\n",
    "\n",
    "        result = self.norm(x) * (scale + 1.0) + shift\n",
    "        g = (gate[0] + 1.0) if self.gate else 1.0\n",
    "        return result, g\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    batch_size, dim, y_dim = 2, 16, 32\n",
    "    x = jax.random.normal(key, (batch_size, dim))\n",
    "    y = jax.random.normal(key, (batch_size, y_dim))\n",
    "\n",
    "    model = AdaLN(dim, y_dim, gate=True, shared=False, rngs=nnx.Rngs(params=42))\n",
    "    output, gate = model(x, y)\n",
    "    print(f'{output.shape = }, {gate.shape = }')\n",
    "\n",
    "    shared_adaln = nnx.Linear(\n",
    "        y_dim,\n",
    "        dim * 3,\n",
    "        use_bias = True,\n",
    "        bias_init = nnx.initializers.zeros_init(),\n",
    "        kernel_init = nnx.initializers.zeros_init(),\n",
    "        rngs = nnx.Rngs(params=43),\n",
    "    )\n",
    "\n",
    "    shared = shared_adaln(y)\n",
    "    scale, shift, *gate = jnp.split(shared, 3, axis=-1)\n",
    "\n",
    "    model = AdaLN(dim, y_dim, gate=True, shared=True, rngs=nnx.Rngs(params=42))\n",
    "    output, gate = model(x, y, shared_adaln=(scale, shift, *gate))\n",
    "    print(f'{output.shape = }, {gate.shape = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd712831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape = (2, 16, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from flax import nnx\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@jax.jit\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., 0::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    x = jnp.stack((-x2, x1), axis=-1)\n",
    "    *shape, d, r = x.shape\n",
    "    return x.reshape(*shape, d * r)\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"start_index\",))\n",
    "def apply_rotary_emb(\n",
    "    freqs: jnp.ndarray,\n",
    "    t: jnp.ndarray,\n",
    "    start_index: int = 0,\n",
    "    scale: float = 1.0\n",
    ") -> jnp.ndarray:\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    end_index = start_index + rot_dim\n",
    "\n",
    "    t_left  = lax.slice_in_dim(t, 0, start_index, axis=-1)\n",
    "    t_mid   = lax.slice_in_dim(t, start_index, end_index, axis=-1)\n",
    "    t_right = lax.slice_in_dim(t, end_index, t.shape[-1], axis=-1)\n",
    "\n",
    "    freqs_cast = jnp.broadcast_to(freqs, t_mid.shape)\n",
    "    t_rot = (t_mid * jnp.cos(freqs_cast) * scale) + (rotate_half(t_mid) * jnp.sin(freqs_cast) * scale)\n",
    "    return jnp.concatenate([t_left, t_rot, t_right], axis=-1)\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"num\",))\n",
    "def centers(start: float, stop: float, num: int, dtype=None):\n",
    "    edges = jnp.linspace(start, stop, num + 1, dtype=dtype)\n",
    "    return (edges[:-1] + edges[1:]) / 2\n",
    "\n",
    "@jax.jit\n",
    "def make_grid(h_pos, w_pos):\n",
    "    grid = jnp.stack(jnp.meshgrid(h_pos, w_pos, indexing=\"ij\"), axis=-1)\n",
    "    return grid.reshape(-1, grid.shape[-1])\n",
    "\n",
    "@jax.jit\n",
    "def bounding_box(h: int, w: int, pixel_aspect_ratio: float = 1.0):\n",
    "    w_adj = w\n",
    "    h_adj = h * pixel_aspect_ratio\n",
    "    ar_adj = w_adj / h_adj\n",
    "    y_min, y_max, x_min, x_max = -1.0, 1.0, -1.0, 1.0\n",
    "    return jnp.select(\n",
    "        [ar_adj > 1, ar_adj < 1],\n",
    "        [jnp.array([-1 / ar_adj, 1 / ar_adj, x_min, x_max]),\n",
    "         jnp.array([y_min, y_max, -ar_adj, ar_adj])],\n",
    "        jnp.array([y_min, y_max, x_min, x_max])\n",
    "    ).astype(jnp.float32)\n",
    "    # return jnp.array([y_min, y_max, x_min, x_max])\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"h\", \"w\", \"align_corners\",))\n",
    "def make_axial_pos(\n",
    "    h: int,\n",
    "    w: int,\n",
    "    pixel_aspect_ratio: float = 1.0,\n",
    "    align_corners: bool = False,\n",
    "    dtype = None,\n",
    "):\n",
    "    y_min, y_max, x_min, x_max = bounding_box(h, w, pixel_aspect_ratio)\n",
    "    if align_corners:\n",
    "        h_pos = jnp.linspace(y_min, y_max, h, dtype=dtype)\n",
    "        w_pos = jnp.linspace(x_min, x_max, w, dtype=dtype)\n",
    "    else:\n",
    "        h_pos = centers(jnp.asarray(y_min), jnp.asarray(y_max), h, dtype=dtype)\n",
    "        w_pos = centers(jnp.asarray(x_min), jnp.asarray(x_max), w, dtype=dtype)\n",
    "    return make_grid(h_pos, w_pos)\n",
    "\n",
    "def freqs_pixel(max_freq: float = 10.0) -> Callable[[tuple], jnp.ndarray]:\n",
    "    def init(shape: tuple) -> jnp.ndarray:\n",
    "        freqs = jnp.linspace(1.0, max_freq / 2.0, shape[-1]) * jnp.pi\n",
    "        return jnp.log(freqs).reshape((1,) * (len(shape) - 1) + (shape[-1],)).repeat(shape[0], 0)\n",
    "    return init\n",
    "\n",
    "def freqs_pixel_log(max_freq: float = 10.0) -> Callable[[tuple], jnp.ndarray]:\n",
    "    def init(shape: tuple) -> jnp.ndarray:\n",
    "        log_min = jnp.log(jnp.pi)\n",
    "        log_max = jnp.log(max_freq * jnp.pi / 2.0)\n",
    "        return jnp.linspace(log_min, log_max, shape[-1]).reshape((1,) * (len(shape) - 1) + (shape[-1],)).repeat(shape[0], 0)\n",
    "    return init\n",
    "\n",
    "class AxialRoPE(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        n_heads: int,\n",
    "        pos_dim: int = 2,\n",
    "        start_index: int = 0,\n",
    "        freqs_init: Callable[[tuple], jnp.ndarray] = freqs_pixel_log(max_freq=10.0),\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.pos_dim = pos_dim\n",
    "        self.start_index = start_index\n",
    "        \n",
    "        per_axis = self.dim // (2 * self.pos_dim)\n",
    "        log_freqs = freqs_init((self.n_heads, per_axis, 1))\n",
    "        self.log_freqs = nnx.Param(log_freqs)\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, pos: jnp.ndarray):\n",
    "        if pos.shape[-1] != self.pos_dim:\n",
    "            raise ValueError(f\"Expected pos_dim={self.pos_dim}, got {pos.shape[-1]}\")\n",
    "\n",
    "        freqs_axes = jnp.exp(self.log_freqs)\n",
    "        freqs_axes = freqs_axes.repeat(self.pos_dim, axis=-1)\n",
    "        freqs = pos[..., None, None, :] * freqs_axes\n",
    "        freqs = freqs.reshape(*freqs.shape[:-2], -1)\n",
    "        freqs = jnp.repeat(freqs, 2, axis=-1)\n",
    "        return apply_rotary_emb(freqs, x, self.start_index)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    batch_size, seq_len, dim = 2, 16, 64\n",
    "    n_heads = 8\n",
    "    pos_dim = 2\n",
    "\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, n_heads, dim // n_heads))\n",
    "\n",
    "    h, w = 4, 4\n",
    "    pos = make_axial_pos(h, w)\n",
    "\n",
    "    model = AxialRoPE(dim=dim // n_heads, n_heads=n_heads, pos_dim=pos_dim, start_index=0)\n",
    "    output = model(x, pos)\n",
    "    print(f'{output.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8ee7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "from flax import nnx\n",
    "\n",
    "class Identity(nnx.Module):\n",
    "    def __call__(self, x):\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size: int = 4,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 512,\n",
    "        *,\n",
    "        norm_layer: Optional[Type[nnx.Module]] = None,\n",
    "        flatten: bool = True,\n",
    "        use_bias: bool = True,\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.flatten = flatten\n",
    "        self.proj = nnx.Conv(\n",
    "            in_features=in_channels,\n",
    "            out_features=embed_dim,\n",
    "            kernel_size=(patch_size, patch_size),\n",
    "            strides=(patch_size, patch_size),\n",
    "            padding='VALID',\n",
    "            use_bias=use_bias,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.norm = Identity() if norm_layer is None else norm_layer(embed_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, pos_map: jnp.ndarray | None = None):\n",
    "        b, _, h, w = x.shape  # NCHW\n",
    "        x = jnp.transpose(x, (0, 2, 3, 1))  # NHWC\n",
    "        x = self.proj(x)                    # (B, H/P, W/P, D)\n",
    "        new_h, new_w = x.shape[1:3]\n",
    "\n",
    "        if pos_map is not None:\n",
    "            pos_map = jax.image.resize(\n",
    "                pos_map, (b, new_h, new_w, pos_map.shape[-1]), method=\"bilinear\"\n",
    "            )\n",
    "            pos_map = pos_map.reshape(b, new_h * new_w, -1)\n",
    "\n",
    "        if self.flatten:\n",
    "            x = x.reshape(b, new_h * new_w, -1)\n",
    "            x = self.norm(x)\n",
    "        else:\n",
    "            x = self.norm(x)\n",
    "            x = jnp.transpose(x, (0, 3, 1, 2))  # NCHW\n",
    "\n",
    "        return x, pos_map\n",
    "\n",
    "\n",
    "class UnPatch(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size: int = 4,\n",
    "        input_dim: int = 512,\n",
    "        out_channels: int = 3,\n",
    "        *,\n",
    "        proj: bool = True,\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        self.patch_size = patch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.linear = (\n",
    "            Identity()\n",
    "            if not proj\n",
    "            else nnx.Linear(\n",
    "                self.input_dim,\n",
    "                self.patch_size**2 * self.out_channels,\n",
    "                rngs=rngs\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        axis1: int | None = None,\n",
    "        axis2: int | None = None,\n",
    "        loss_mask: jnp.ndarray | None = None,\n",
    "    ):\n",
    "        b, n, _ = x.shape\n",
    "        p = q = self.patch_size\n",
    "\n",
    "        if axis1 is None and axis2 is None:\n",
    "            w = h = int(jnp.sqrt(n).item())\n",
    "            assert h * w == n\n",
    "        else:\n",
    "            h = axis1 // p if axis1 else n // (axis2 // p)\n",
    "            w = axis2 // p if axis2 else n // h\n",
    "            assert h * w == n\n",
    "\n",
    "        x = self.linear(x)  # (B, N, p^2 * C)\n",
    "\n",
    "        if loss_mask is not None:\n",
    "            loss_mask = loss_mask[..., None]\n",
    "            x = jnp.where(loss_mask, x, lax.stop_gradient(x))\n",
    "\n",
    "        x = x.reshape(b, h, w, p, q, self.out_channels)\n",
    "        x = x.transpose(0, 5, 1, 3, 2, 4)\n",
    "        x = x.reshape(b, self.out_channels, h * p, w * q)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75f484d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "class TimestepEmbedding(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        max_period: float = 10000.0,\n",
    "        time_factor: float = 1000.0,\n",
    "        *,\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.max_period = max_period\n",
    "        self.time_factor = time_factor\n",
    "        self.freqs = jnp.exp(-jnp.log(self.max_period) * jnp.arange(0, self.dim // 2, dtype=jnp.float32) / (self.dim // 2))[None, :]\n",
    "        self.proj = nnx.Linear(self.dim, self.dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, t: jnp.ndarray) -> jnp.ndarray:\n",
    "        if t.ndim > 1:\n",
    "            t = jnp.squeeze(t, -1)\n",
    "        t = self.time_factor * t\n",
    "        args = t[:, None] * self.freqs  # (B, dim//2)\n",
    "        embedding = jnp.concatenate([jnp.cos(args), jnp.sin(args)], axis=-1)\n",
    "\n",
    "        # padding\n",
    "        if self.dim % 2 == 1:\n",
    "            embedding = jnp.concatenate(\n",
    "                [embedding, jnp.zeros((embedding.shape[0], 1), dtype=embedding.dtype)], axis=-1\n",
    "            )\n",
    "\n",
    "        x = self.proj(embedding)\n",
    "        x = jax.nn.mish(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec428b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa_out.shape = (2, 16, 64)\n",
      "ca_out.shape = (2, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Union, Callable\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "#from .axial_rope import AxialRoPE\n",
    "\n",
    "MaskType = Union[jnp.ndarray, None]\n",
    "\n",
    "def _split_heads(x, n_heads):\n",
    "    B, N, D = x.shape\n",
    "    H = n_heads\n",
    "    return x.reshape(B, N, H, D // H)\n",
    "\n",
    "def _merge_heads(x):\n",
    "    B, N, H, D = x.shape\n",
    "    return x.reshape(B, N, H * D)\n",
    "\n",
    "def _prepare_mask_bias(mask: MaskType, B: int, H: int, q_len: int, kv_len: int):\n",
    "    if mask is None:\n",
    "        return {'mask': None, 'bias': None}\n",
    "    arr = jnp.asarray(mask)\n",
    "\n",
    "    def _broadcast(arr):\n",
    "        # Supported input shapes:\n",
    "        #  - (q_len, kv_len): e.g. causal mask shared across batch\n",
    "        #  - (B, kv_len): padding mask\n",
    "        #  - (B, q_len, kv_len): per-example mask\n",
    "        #  - (B, H, q_len, kv_len): per-head mask/bias\n",
    "        if arr.ndim == 2:\n",
    "            if arr.shape == (B, kv_len):\n",
    "                arr = arr[:, None, None, :]\n",
    "            else:\n",
    "                arr = arr[None, None, :, :]\n",
    "        elif arr.ndim == 3:\n",
    "            arr = arr[:, None, :, :]\n",
    "        elif arr.ndim != 4:\n",
    "            raise ValueError(f\"Unsupported mask/bias shape {arr.shape}\")\n",
    "        return jnp.broadcast_to(arr, (B, H, q_len, kv_len))\n",
    "\n",
    "    if arr.dtype == jnp.bool_:\n",
    "        return {'mask': _broadcast(arr), 'bias': None}\n",
    "    return {'mask': None, 'bias': _broadcast(arr)}\n",
    "\n",
    "class SelfAttention(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        n_heads: int = 8,\n",
    "        head_dim: int = -1,\n",
    "        pos_dim: int = 2,\n",
    "        *,\n",
    "        rope_factory: Optional[Callable[[int, int, int], nnx.Module]] = None, # AxialRoPE alternative\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        self.head_dim = head_dim if head_dim > 0 else dim // n_heads\n",
    "        self.pos_dim = pos_dim\n",
    "        self.n_heads = dim // self.head_dim\n",
    "        self.qkv = nnx.Linear(dim, dim * 3, use_bias=False, rngs=rngs)\n",
    "        self.out = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "        factory = rope_factory or (lambda d, h, p: AxialRoPE(d, h, p))\n",
    "        self.rope = factory(self.head_dim, self.n_heads, self.pos_dim)\n",
    "\n",
    "    def __call__(self, x, pos_map=None, mask=None, deterministic=True):\n",
    "        b, n, _ = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = jnp.split(qkv, 3, axis=-1)\n",
    "\n",
    "        q = _split_heads(q, self.n_heads)\n",
    "        k = _split_heads(k, self.n_heads)\n",
    "        v = _split_heads(v, self.n_heads)\n",
    "\n",
    "        if pos_map is not None:\n",
    "            # pos_map expected shape: (b, n, pos_dim)\n",
    "            q = self.rope(q, pos_map)\n",
    "            k = self.rope(k, pos_map)\n",
    "\n",
    "        mask_bias = _prepare_mask_bias(mask, b, self.n_heads, n, n)\n",
    "        attn = nnx.dot_product_attention(\n",
    "            q, k, v,\n",
    "            deterministic=deterministic,\n",
    "            **mask_bias,\n",
    "        )\n",
    "        attn = _merge_heads(attn)\n",
    "        return self.out(attn)\n",
    "\n",
    "class CrossAttention(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        ctx_dim: int,\n",
    "        n_heads: int = 8,\n",
    "        head_dim: int = -1,\n",
    "        pos_dim: int = 2,\n",
    "        *,\n",
    "        rope_factory: Optional[Callable[[int, int, int], nnx.Module]] = None, # AxialRoPE alternative\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        self.head_dim = head_dim if head_dim > 0 else dim // n_heads\n",
    "        self.pos_dim = pos_dim\n",
    "        self.n_heads = dim // self.head_dim\n",
    "        self.q = nnx.Linear(dim, dim, use_bias=False, rngs=rngs)\n",
    "        self.kv = nnx.Linear(ctx_dim, dim * 2, use_bias=False, rngs=rngs)\n",
    "        self.out = nnx.Linear(dim, dim, rngs=rngs)\n",
    "\n",
    "        factory = rope_factory or (lambda d, h, p: AxialRoPE(d, h, p))\n",
    "        self.rope = factory(self.head_dim, self.n_heads, self.pos_dim)\n",
    "\n",
    "    def __call__(self, x, ctx, pos_map=None, ctx_pos_map=None, mask=None, deterministic=True):\n",
    "        b, n, _ = x.shape\n",
    "        ctx_n = ctx.shape[1]\n",
    "\n",
    "        q = _split_heads(self.q(x), self.n_heads)\n",
    "        k, v = jnp.split(self.kv(ctx), 2, axis=-1)\n",
    "        k = _split_heads(k, self.n_heads)\n",
    "        v = _split_heads(v, self.n_heads)\n",
    "\n",
    "        if pos_map is not None:\n",
    "            q = self.rope(q, pos_map)\n",
    "        if ctx_pos_map is not None:\n",
    "            k = self.rope(k, ctx_pos_map)\n",
    "\n",
    "        mask_bias = _prepare_mask_bias(mask, b, self.n_heads, n, ctx_n)\n",
    "\n",
    "        attn = nnx.dot_product_attention(\n",
    "            q, k, v,\n",
    "            deterministic=deterministic,\n",
    "            **mask_bias,\n",
    "        )\n",
    "        attn = _merge_heads(attn)\n",
    "        return self.out(attn)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    key, pkey_sa, pkey_ca, xkey, ckey = jax.random.split(key, 5)\n",
    "\n",
    "    B, N, D = 2, 16, 64\n",
    "    x = jax.random.normal(xkey, (B, N, D))\n",
    "    ctx = jax.random.normal(ckey, (B, N, D))\n",
    "\n",
    "    sa = SelfAttention(dim=D, n_heads=8, rngs=nnx.Rngs(pkey_sa))\n",
    "    sa_out = sa(x)\n",
    "    print(f'{sa_out.shape = }')\n",
    "\n",
    "    ca = CrossAttention(dim=D, ctx_dim=D, n_heads=8, rngs=nnx.Rngs(pkey_ca))\n",
    "    ca_out = ca(x, ctx)\n",
    "    print(f'{ca_out.shape = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "afa4410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Block\n",
      "(2, 16, 64) -> (2, 16, 64)\n",
      "Cross-Attention Block\n",
      "(2, 16, 64) -> (2, 16, 64)\n",
      "AdaLN Block\n",
      "(2, 16, 64) -> (2, 16, 64)\n",
      "Shared AdaLN Block\n",
      "(2, 16, 64) -> (2, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Type\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "# from .layers import SwiGLU\n",
    "# from .attention import SelfAttention, CrossAttention\n",
    "# from .adaln import AdaLN\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        ctx_dim: Optional[int],\n",
    "        heads: int,\n",
    "        dim_head: int,\n",
    "        mlp_dim: int,\n",
    "        pos_dim: int,\n",
    "        *,\n",
    "        use_adaln: bool = False,\n",
    "        use_shared_adaln: bool = False,\n",
    "        ctx_from_self: bool = False,\n",
    "        norm_layer: Type[nnx.Module] = nnx.RMSNorm,\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        self.use_adaln = use_adaln\n",
    "        self.ctx_from_self = ctx_from_self\n",
    "        self.sa = SelfAttention(dim, heads, dim_head, pos_dim, rngs=rngs)\n",
    "        self.xa = CrossAttention(dim, ctx_dim, heads, dim_head, pos_dim, rngs=rngs) if ctx_dim is not None else None\n",
    "        self.mlp = SwiGLU(dim, mlp_dim, dim, rngs=rngs)\n",
    "\n",
    "        if self.use_adaln:\n",
    "            self.sa_pre = AdaLN(dim, dim, norm_layer=norm_layer, shared=use_shared_adaln, rngs=rngs)\n",
    "            self.xa_pre = AdaLN(dim, dim, norm_layer=norm_layer, shared=use_shared_adaln, rngs=rngs) if self.xa else None\n",
    "            self.mlp_pre = AdaLN(dim, dim, norm_layer=norm_layer, shared=use_shared_adaln, rngs=rngs)\n",
    "        else:\n",
    "            self.sa_pre = norm_layer(dim, rngs=rngs)\n",
    "            self.xa_pre = norm_layer(dim, rngs=rngs) if self.xa else None\n",
    "            self.mlp_pre = norm_layer(dim, rngs=rngs)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x,\n",
    "        ctx=None,\n",
    "        pos_map=None,\n",
    "        ctx_pos_map=None,\n",
    "        y=None,\n",
    "        x_mask=None,\n",
    "        ctx_mask=None,\n",
    "        shared_adaln=None,\n",
    "        deterministic: bool = True,\n",
    "    ):\n",
    "        # Self-Attention\n",
    "        if self.use_adaln:\n",
    "            sa_shared = shared_adaln[0] if shared_adaln is not None else None\n",
    "            normed_x, gate = self.sa_pre(x, y, shared_adaln=sa_shared)\n",
    "        else:\n",
    "            normed_x = self.sa_pre(x)\n",
    "            gate = jnp.ones_like(normed_x)\n",
    "        if gate.ndim < x.ndim:\n",
    "            gate = jnp.expand_dims(gate, axis=tuple(range(1, x.ndim - gate.ndim + 1)))\n",
    "        x = x + gate * self.sa(\n",
    "            normed_x,\n",
    "            pos_map=pos_map,\n",
    "            mask=x_mask,\n",
    "            deterministic=deterministic\n",
    "        )\n",
    "\n",
    "        # Cross-Attention\n",
    "        if self.xa is not None:\n",
    "            if self.use_adaln:\n",
    "                xa_shared = shared_adaln[1] if shared_adaln is not None else None\n",
    "                normed_x, gate = self.xa_pre(x, y, shared_adaln=xa_shared)\n",
    "            else:\n",
    "                normed_x = self.xa_pre(x)\n",
    "                gate = jnp.ones_like(normed_x)\n",
    "            if gate.ndim < x.ndim:\n",
    "                gate = jnp.expand_dims(gate, axis=tuple(range(1, x.ndim - gate.ndim + 1)))\n",
    "            xa_ctx = x if self.ctx_from_self else ctx\n",
    "            xa_mask = x_mask if self.ctx_from_self else ctx_mask\n",
    "            x = x + gate * self.xa(\n",
    "                normed_x, xa_ctx,\n",
    "                pos_map=pos_map,\n",
    "                ctx_pos_map=ctx_pos_map,\n",
    "                mask=xa_mask,\n",
    "                deterministic=deterministic\n",
    "            )\n",
    "\n",
    "        # MLP\n",
    "        if self.use_adaln:\n",
    "            mlp_shared = shared_adaln[2] if shared_adaln is not None else None\n",
    "            normed_x, gate = self.mlp_pre(x, y, shared_adaln=mlp_shared)\n",
    "        else:\n",
    "            normed_x = self.mlp_pre(x)\n",
    "            gate = jnp.ones_like(normed_x)\n",
    "        if gate.ndim < x.ndim:\n",
    "            gate = jnp.expand_dims(gate, axis=tuple(range(1, x.ndim - gate.ndim + 1)))\n",
    "        x = x + gate * self.mlp(normed_x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    dim = 64\n",
    "    y_dim = 64\n",
    "    mlp_dim = 128\n",
    "    heads = 8\n",
    "    head_dim = 8\n",
    "    pos_dim = 2\n",
    "\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, dim))\n",
    "    ctx = jax.random.normal(key, (batch_size, seq_len, y_dim))\n",
    "\n",
    "    print(\"Self-Attention Block\")\n",
    "    sa_block = TransformerBlock(\n",
    "        dim=dim,\n",
    "        ctx_dim=None,  # w/o ctx\n",
    "        heads=heads,\n",
    "        dim_head=head_dim,\n",
    "        mlp_dim=mlp_dim,\n",
    "        pos_dim=pos_dim,\n",
    "        rngs=nnx.Rngs(key),\n",
    "    )\n",
    "    sa_out = sa_block(x, deterministic=True)\n",
    "    print(x.shape, \"->\", sa_out.shape)\n",
    "    assert sa_out.shape == x.shape\n",
    "\n",
    "    print(\"Cross-Attention Block\")\n",
    "    xa_block = TransformerBlock(\n",
    "        dim=dim,\n",
    "        ctx_dim=y_dim,  # w/ ctx\n",
    "        heads=heads,\n",
    "        dim_head=head_dim,\n",
    "        mlp_dim=mlp_dim,\n",
    "        pos_dim=pos_dim,\n",
    "        rngs=nnx.Rngs(key),\n",
    "    )\n",
    "    xa_out = xa_block(x, ctx=ctx, deterministic=True)\n",
    "    print(x.shape, \"->\", xa_out.shape)\n",
    "    assert xa_out.shape == x.shape\n",
    "\n",
    "    print(\"AdaLN Block\")\n",
    "    adaln_block = TransformerBlock(\n",
    "        dim=dim,\n",
    "        ctx_dim=y_dim,\n",
    "        heads=heads,\n",
    "        dim_head=head_dim,\n",
    "        mlp_dim=mlp_dim,\n",
    "        pos_dim=pos_dim,\n",
    "        use_adaln=True,\n",
    "        use_shared_adaln=False,\n",
    "        rngs=nnx.Rngs(key),\n",
    "    )\n",
    "    adaln_out = adaln_block(x, ctx=ctx, y=x, deterministic=True)\n",
    "    print(x.shape, \"->\", adaln_out.shape)\n",
    "    assert adaln_out.shape == x.shape\n",
    "\n",
    "    print(\"Shared AdaLN Block\")\n",
    "    shared_adaln_block = TransformerBlock(\n",
    "        dim=dim,\n",
    "        ctx_dim=y_dim,\n",
    "        heads=heads,\n",
    "        dim_head=head_dim,\n",
    "        mlp_dim=mlp_dim,\n",
    "        pos_dim=pos_dim,\n",
    "        use_adaln=True,\n",
    "        use_shared_adaln=True,\n",
    "        rngs=nnx.Rngs(key),\n",
    "    )\n",
    "    shared_adaln = (\n",
    "        (jnp.zeros_like(x), jnp.zeros_like(x), jnp.zeros_like(x)),  # sa\n",
    "        (jnp.zeros_like(x), jnp.zeros_like(x), jnp.zeros_like(x)),  # xa\n",
    "        (jnp.zeros_like(x), jnp.zeros_like(x), jnp.zeros_like(x)),  # mlp\n",
    "    )\n",
    "    out_shared = shared_adaln_block(\n",
    "        x,\n",
    "        ctx=ctx,\n",
    "        y=x,\n",
    "        shared_adaln=shared_adaln,\n",
    "        deterministic=True,\n",
    "    )\n",
    "    print(x.shape, \"->\", out_shared.shape)\n",
    "    assert out_shared.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence, Tuple, List\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from flax.nnx import RMSNorm\n",
    "\n",
    "# from .modules.transformer import TransformerBlock\n",
    "# from .modules.patch import PatchEmbed, UnPatch\n",
    "# from .modules.axial_rope import make_axial_pos\n",
    "# from .modules.time_emb import TimestepEmbedding\n",
    "\n",
    "def _isiterable(x) -> bool:\n",
    "    try:\n",
    "        iter(x)\n",
    "    except TypeError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class _SharedAdaLNHead(nnx.Module):\n",
    "    def __init__(self, dim: int, *, rngs: nnx.rnglib.Rngs):\n",
    "        self.dim = dim\n",
    "        self.norm = nnx.LayerNorm(num_features=dim, rngs=rngs)\n",
    "        self.fc1 = nnx.Linear(dim, dim * 4, rngs=rngs)\n",
    "        self.fc2 = nnx.Linear(\n",
    "            dim * 4,\n",
    "            dim * 3,\n",
    "            kernel_init=nnx.initializers.zeros_init(),\n",
    "            bias_init=nnx.initializers.zeros_init(),\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        y = self.norm(x)\n",
    "        y = self.fc1(y)\n",
    "        y = jax.nn.mish(y)\n",
    "        y = self.fc2(y)\n",
    "        return y  # (B, dim*3)\n",
    "\n",
    "def _chunk3(v: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    d3 = v.shape[-1]\n",
    "    assert d3 % 3 == 0\n",
    "    d = d3 // 3\n",
    "    return v[..., :d], v[..., d:2*d], v[..., 2*d:]\n",
    "\n",
    "class TBackBone(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 1024,\n",
    "        ctx_dim: Optional[int] = 1024,\n",
    "        heads: int = 16,\n",
    "        dim_head: int = 64,\n",
    "        mlp_dim: int = 3072,\n",
    "        pos_dim: int = 2,\n",
    "        depth: int = 8,\n",
    "        use_adaln: bool = False,\n",
    "        use_shared_adaln: bool = False,\n",
    "        use_dyt: bool = False,\n",
    "        grad_ckpt: bool = False,  # remat (not used)\n",
    "        *,\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        norm_layer = RMSNorm\n",
    "        self.use_adaln = use_adaln\n",
    "        self.use_shared_adaln = use_shared_adaln\n",
    "        self.blocks = tuple([\n",
    "            TransformerBlock(\n",
    "                dim=dim,\n",
    "                ctx_dim=ctx_dim,\n",
    "                heads=heads,\n",
    "                dim_head=dim_head,\n",
    "                mlp_dim=mlp_dim,\n",
    "                pos_dim=pos_dim,\n",
    "                use_adaln=use_adaln,\n",
    "                use_shared_adaln=use_shared_adaln,\n",
    "                norm_layer=norm_layer,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        ctx: Optional[jnp.ndarray] = None,\n",
    "        x_mask: Optional[jnp.ndarray] = None,\n",
    "        ctx_mask: Optional[jnp.ndarray] = None,\n",
    "        pos_map: Optional[jnp.ndarray] = None,\n",
    "        y: Optional[jnp.ndarray] = None,\n",
    "        shared_adaln: Optional[Sequence[Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]]] = None,\n",
    "        deterministic: bool = True,\n",
    "    ) -> jnp.ndarray:\n",
    "        for block in self.blocks:\n",
    "            x = block(\n",
    "                x,\n",
    "                ctx=ctx,\n",
    "                pos_map=pos_map,\n",
    "                ctx_pos_map=None,\n",
    "                y=y,\n",
    "                x_mask=x_mask,\n",
    "                ctx_mask=ctx_mask,\n",
    "                shared_adaln=shared_adaln,\n",
    "                deterministic=deterministic\n",
    "            )\n",
    "        return x\n",
    "\n",
    "class XUTBackBone(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int = 1024,\n",
    "        ctx_dim: Optional[int] = None,\n",
    "        heads: int = 16,\n",
    "        dim_head: int = 64,\n",
    "        mlp_dim: int = 3072,\n",
    "        pos_dim: int = 2,\n",
    "        depth: int = 8,\n",
    "        enc_blocks: int | Sequence[int] = 1,\n",
    "        dec_blocks: int | Sequence[int] = 2,\n",
    "        dec_ctx: bool = False,\n",
    "        use_adaln: bool = False,\n",
    "        use_shared_adaln: bool = False,\n",
    "        use_dyt: bool = False,\n",
    "        grad_ckpt: bool = False,\n",
    "        *,\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        norm_layer = RMSNorm\n",
    "        self.dim = dim\n",
    "        self.ctx_dim = ctx_dim\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.pos_dim = pos_dim\n",
    "        self.depth = depth\n",
    "        self.dec_ctx = dec_ctx\n",
    "        self.use_adaln = use_adaln\n",
    "        self.use_shared_adaln = use_shared_adaln\n",
    "\n",
    "        if _isiterable(enc_blocks):\n",
    "            enc_list = list(enc_blocks)\n",
    "            assert len(enc_list) == depth\n",
    "        else:\n",
    "            enc_list = [int(enc_blocks)] * depth\n",
    "\n",
    "        if _isiterable(dec_blocks):\n",
    "            dec_list = list(dec_blocks)\n",
    "            assert len(dec_list) == depth\n",
    "        else:\n",
    "            dec_list = [int(dec_blocks)] * depth\n",
    "\n",
    "        def mk_block(ctx_dim_inner, ctx_from_self: bool = False):\n",
    "            return TransformerBlock(\n",
    "                dim=dim,\n",
    "                ctx_dim=ctx_dim_inner,\n",
    "                heads=heads,\n",
    "                dim_head=dim_head,\n",
    "                mlp_dim=mlp_dim,\n",
    "                pos_dim=pos_dim,\n",
    "                use_adaln=use_adaln,\n",
    "                use_shared_adaln=use_shared_adaln,\n",
    "                ctx_from_self=ctx_from_self,\n",
    "                norm_layer=norm_layer,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "\n",
    "        self.enc_stacks = tuple([\n",
    "            tuple(mk_block(ctx_dim) for _ in range(enc_list[i]))\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.dec_stacks = tuple([\n",
    "            tuple(\n",
    "                mk_block(dim, ctx_from_self=True) if bid == 0\n",
    "                else mk_block(ctx_dim if dec_ctx else None)\n",
    "                for bid in range(dec_list[i])\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        ctx: Optional[jnp.ndarray] = None,\n",
    "        x_mask: Optional[jnp.ndarray] = None,\n",
    "        ctx_mask: Optional[jnp.ndarray] = None,\n",
    "        pos_map: Optional[jnp.ndarray] = None,\n",
    "        y: Optional[jnp.ndarray] = None,\n",
    "        shared_adaln: Optional[Sequence[Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]]] = None,\n",
    "        return_enc_out: bool = False,\n",
    "        deterministic: bool = True,\n",
    "    ) -> jnp.ndarray | Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        self_ctx: List[jnp.ndarray] = []\n",
    "        for blocks in self.enc_stacks:\n",
    "            for block in blocks:\n",
    "                x = block(x,\n",
    "                        ctx=ctx,\n",
    "                        pos_map=pos_map,\n",
    "                        ctx_pos_map=None,\n",
    "                        y=y,\n",
    "                        x_mask=x_mask,\n",
    "                        ctx_mask=ctx_mask,\n",
    "                        shared_adaln=shared_adaln,\n",
    "                        deterministic=deterministic)\n",
    "            self_ctx.append(x)\n",
    "        enc_out = x\n",
    "\n",
    "        for blocks in self.dec_stacks:\n",
    "            # first block cross attends to last encoder output\n",
    "            first = blocks[0]\n",
    "            x = first(x,\n",
    "                      ctx=self_ctx[-1],\n",
    "                      pos_map=pos_map,\n",
    "                      ctx_pos_map=pos_map,\n",
    "                      y=y,\n",
    "                      x_mask=x_mask,\n",
    "                      ctx_mask=ctx_mask,\n",
    "                      shared_adaln=shared_adaln,\n",
    "                      deterministic=deterministic)\n",
    "            for block in blocks[1:]:\n",
    "                x = block(x,\n",
    "                        ctx=ctx if self.dec_ctx else None,\n",
    "                        pos_map=pos_map,\n",
    "                        ctx_pos_map=None,\n",
    "                        y=y,\n",
    "                        x_mask=x_mask,\n",
    "                        ctx_mask=ctx_mask,\n",
    "                        shared_adaln=shared_adaln,\n",
    "                        deterministic=deterministic)\n",
    "\n",
    "        if return_enc_out:\n",
    "            return x, enc_out\n",
    "        return x\n",
    "\n",
    "class XUDiT(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size: int = 2,\n",
    "        input_dim: int = 4,\n",
    "        dim: int = 1024,\n",
    "        ctx_dim: Optional[int] = 1024,\n",
    "        ctx_size: int = 256,\n",
    "        heads: int = 16,\n",
    "        dim_head: int = 64,\n",
    "        mlp_dim: int = 3072,\n",
    "        depth: int = 8,\n",
    "        enc_blocks: int | Sequence[int] = 1,\n",
    "        dec_blocks: int | Sequence[int] = 2,\n",
    "        dec_ctx: bool = False,\n",
    "        class_cond: int = 0,\n",
    "        shared_adaln: bool = True,\n",
    "        concat_ctx: bool = True,\n",
    "        use_dyt: bool = False,\n",
    "        double_t: bool = False,\n",
    "        addon_info_embs_dim: Optional[int] = None,\n",
    "        tread_config: Optional[dict] = None,\n",
    "        grad_ckpt: bool = False,\n",
    "        *,\n",
    "        rngs: nnx.rnglib.Rngs,\n",
    "    ):\n",
    "        self.rngs = rngs\n",
    "        self.patch_size = patch_size\n",
    "        self.input_dim = input_dim\n",
    "        self.dim = dim\n",
    "        self.ctx_dim = ctx_dim\n",
    "        self.ctx_size = ctx_size\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.depth = depth\n",
    "        self.enc_blocks = enc_blocks\n",
    "        self.dec_blocks = dec_blocks\n",
    "        self.dec_ctx = dec_ctx\n",
    "        self.class_cond = class_cond\n",
    "        self.shared_adaln = shared_adaln\n",
    "        self.concat_ctx = concat_ctx\n",
    "        self.use_dyt = use_dyt\n",
    "        self.double_t = double_t\n",
    "        self.addon_info_embs_dim = addon_info_embs_dim\n",
    "        self.tread_config = tread_config\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "\n",
    "        self.backbone = XUTBackBone(\n",
    "            dim=self.dim,\n",
    "            ctx_dim=(None if self.concat_ctx else self.ctx_dim),\n",
    "            heads=self.heads,\n",
    "            dim_head=self.dim_head,\n",
    "            mlp_dim=self.mlp_dim,\n",
    "            pos_dim=2,\n",
    "            depth=self.depth,\n",
    "            enc_blocks=self.enc_blocks,\n",
    "            dec_blocks=self.dec_blocks,\n",
    "            dec_ctx=self.dec_ctx,\n",
    "            use_adaln=True,\n",
    "            use_shared_adaln=self.shared_adaln,\n",
    "            use_dyt=self.use_dyt,\n",
    "            grad_ckpt=self.grad_ckpt,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "        self.use_tread = self.tread_config is not None\n",
    "        if self.use_tread:\n",
    "            dr = float(self.tread_config[\"dropout_ratio\"])\n",
    "            prev_d = int(self.tread_config[\"prev_trns_depth\"])\n",
    "            post_d = int(self.tread_config[\"post_trns_depth\"])\n",
    "            self.dropout_ratio = dr\n",
    "            self.prev_tread_trns = TBackBone(\n",
    "                dim=self.dim,\n",
    "                ctx_dim=(None if self.concat_ctx else self.ctx_dim),\n",
    "                heads=self.heads,\n",
    "                dim_head=self.dim_head,\n",
    "                mlp_dim=self.mlp_dim,\n",
    "                pos_dim=2,\n",
    "                depth=prev_d,\n",
    "                use_adaln=True,\n",
    "                use_shared_adaln=self.shared_adaln,\n",
    "                use_dyt=self.use_dyt,\n",
    "                grad_ckpt=self.grad_ckpt,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "            self.post_tread_trns = TBackBone(\n",
    "                dim=self.dim,\n",
    "                ctx_dim=(None if self.concat_ctx else self.ctx_dim),\n",
    "                heads=self.heads,\n",
    "                dim_head=self.dim_head,\n",
    "                mlp_dim=self.mlp_dim,\n",
    "                pos_dim=2,\n",
    "                depth=post_d,\n",
    "                use_adaln=True,\n",
    "                use_shared_adaln=self.shared_adaln,\n",
    "                use_dyt=self.use_dyt,\n",
    "                grad_ckpt=self.grad_ckpt,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "\n",
    "        self.patch_size_ = self.patch_size\n",
    "        self.in_patch = PatchEmbed(\n",
    "            patch_size=self.patch_size,\n",
    "            in_channels=self.input_dim,\n",
    "            embed_dim=self.dim,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.out_patch = UnPatch(\n",
    "            patch_size=self.patch_size,\n",
    "            input_dim=self.dim,\n",
    "            out_channels=self.input_dim,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "        self.time_emb = TimestepEmbedding(self.dim, rngs=rngs)\n",
    "        if self.double_t:\n",
    "            self.r_emb = TimestepEmbedding(self.dim, rngs=rngs)\n",
    "\n",
    "        if self.shared_adaln:\n",
    "            self.shared_adaln_attn = _SharedAdaLNHead(self.dim, rngs=rngs)\n",
    "            self.shared_adaln_xattn = _SharedAdaLNHead(self.dim, rngs=rngs)\n",
    "            self.shared_adaln_ffw = _SharedAdaLNHead(self.dim, rngs=rngs)\n",
    "\n",
    "        if self.class_cond > 0:\n",
    "            self.class_token = nnx.Embed(num_embeddings=self.class_cond, features=self.dim, rngs=rngs)\n",
    "        else:\n",
    "            self.class_token = None\n",
    "\n",
    "        if self.concat_ctx and (self.ctx_dim is not None):\n",
    "            self.ctx_proj = nnx.Linear(self.ctx_dim, self.dim, rngs=rngs)\n",
    "        else:\n",
    "            self.ctx_proj = None\n",
    "\n",
    "        if self.addon_info_embs_dim is not None:\n",
    "            self.addon_info_embs_proj_1 = nnx.Linear(self.addon_info_embs_dim, self.dim, rngs=rngs)\n",
    "            self.addon_info_embs_proj_2 = nnx.Linear(\n",
    "                self.dim,\n",
    "                self.dim,\n",
    "                kernel_init=nnx.initializers.zeros_init(),\n",
    "                bias_init=nnx.initializers.zeros_init(),\n",
    "                rngs=rngs,\n",
    "            )\n",
    "\n",
    "    def _make_shared_adaln_state(self, t_emb: jnp.ndarray):\n",
    "        attn = self.shared_adaln_attn(t_emb)\n",
    "        xattn = self.shared_adaln_xattn(t_emb)\n",
    "        ffw = self.shared_adaln_ffw(t_emb)\n",
    "        return [_chunk3(attn), _chunk3(xattn), _chunk3(ffw)]\n",
    "\n",
    "    def __call__(self,\n",
    "        x: jnp.ndarray,                           # NHWC (B, H, W, C)\n",
    "        t: jnp.ndarray,                           # (B,) | (B, 1)\n",
    "        ctx: Optional[jnp.ndarray] = None,        # (B, T_ctx, ctx_dim) | None | (B,) class ids if class_cond>0\n",
    "        pos_map: Optional[jnp.ndarray] = None,    # (B, N, 2)\n",
    "        r: Optional[jnp.ndarray] = None,\n",
    "        addon_info: Optional[jnp.ndarray] = None, # (B, D_addon) or (B,)\n",
    "        tread_rate: Optional[float] = None,\n",
    "        return_enc_out: bool = False,\n",
    "        deterministic: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        -> (B, C, H, W)\n",
    "        \"\"\"\n",
    "        B, H, W, C = x.shape\n",
    "        x_seq, pos_map_resized = self.in_patch(x, pos_map)  # x_seq: (B, N, D), pos_map_resized: (B, N, 2) or None\n",
    "        N = x_seq.shape[1]\n",
    "        if pos_map_resized is None:\n",
    "            pos_map_resized = make_axial_pos(H // self.patch_size_,\n",
    "                                             W // self.patch_size_)\n",
    "            pos_map_resized = jnp.broadcast_to(pos_map_resized[None, ...],\n",
    "                                               (B, pos_map_resized.shape[0], pos_map_resized.shape[1]))\n",
    "\n",
    "        t_emb = self.time_emb(t)\n",
    "        if r is not None:\n",
    "            r = jnp.reshape(r, (B, -1))\n",
    "            t_emb = t_emb + self.r_emb((t.reshape(B, -1) - r))\n",
    "\n",
    "        if (self.class_token is not None) and (ctx is not None):\n",
    "            if ctx.ndim == 1:\n",
    "                cls_ids = ctx\n",
    "            else:\n",
    "                cls_ids = ctx.reshape((ctx.shape[0], -1))[:, 0]\n",
    "            t_emb = t_emb + self.class_token(cls_ids)\n",
    "            ctx = None\n",
    "\n",
    "        if addon_info is not None:\n",
    "            if addon_info.ndim == 1:\n",
    "                addon_info = addon_info[:, None]\n",
    "            addon_embs = jax.nn.mish(self.addon_info_embs_proj_1(addon_info))\n",
    "            addon_embs = self.addon_info_embs_proj_2(addon_embs)\n",
    "            addon_embs = addon_embs[:, None, :]  # (B,1,D)\n",
    "            t_emb = t_emb + addon_embs\n",
    "\n",
    "        need_ctx = (self.ctx_dim is not None)\n",
    "        if (ctx is None) and need_ctx and (not self.concat_ctx):\n",
    "            ctx = jnp.zeros((B, self.ctx_size, self.ctx_dim), dtype=x_seq.dtype)\n",
    "\n",
    "        shared_adaln_state = None\n",
    "        if self.shared_adaln:\n",
    "            shared_adaln_state = self._make_shared_adaln_state(t_emb)\n",
    "\n",
    "        length = x_seq.shape[1]\n",
    "\n",
    "        if self.ctx_proj is not None and ctx is not None:\n",
    "            ctx_proj = self.ctx_proj(ctx)\n",
    "            x_seq = jnp.concatenate([x_seq, ctx_proj], axis=1)\n",
    "            pad_pos = jnp.zeros((B, ctx_proj.shape[1], pos_map_resized.shape[-1]), dtype=pos_map_resized.dtype)\n",
    "            pos_map_resized = jnp.concatenate([pos_map_resized, pad_pos], axis=1)\n",
    "            ctx = None\n",
    "\n",
    "        # TREAD (pre)\n",
    "        if self.use_tread:\n",
    "            x_seq = self.prev_tread_trns(\n",
    "                x_seq, ctx=ctx, pos_map=pos_map_resized, y=t_emb,\n",
    "                shared_adaln=shared_adaln_state, deterministic=deterministic\n",
    "            )\n",
    "            do_tread = (not deterministic) or (tread_rate is not None)\n",
    "            if do_tread:\n",
    "                rate = (tread_rate if tread_rate is not None else self.dropout_ratio)\n",
    "                keep_len = length - int(length * rate)\n",
    "                key = self.rngs.dropout()\n",
    "                perm = jax.vmap(lambda k: jax.random.permutation(k, length))(jax.random.split(key, B))\n",
    "                sel_mask = perm < keep_len\n",
    "                if self.ctx_proj is not None:\n",
    "                    ctx_len = x_seq.shape[1] - length\n",
    "                    ctx_keep = jnp.ones((B, ctx_len), dtype=sel_mask.dtype)\n",
    "                    sel_mask = jnp.concatenate([sel_mask, ctx_keep], axis=1)\n",
    "                    keep_len = keep_len + ctx_len\n",
    "\n",
    "                def _gather_sel(x_row, m_row):\n",
    "                    idx = jnp.nonzero(m_row, size=m_row.shape[0], fill_value=0)[0]\n",
    "                    idx = idx[:keep_len]\n",
    "                    return x_row[idx]\n",
    "                x_not_sel = jnp.where(sel_mask[..., None], jnp.zeros_like(x_seq), x_seq)\n",
    "                x_sel = jax.vmap(_gather_sel)(x_seq, sel_mask)\n",
    "                x_seq = x_sel\n",
    "                raw_pos = pos_map_resized\n",
    "                pos_map_resized = jax.vmap(_gather_sel)(pos_map_resized, sel_mask)\n",
    "\n",
    "        # Backbone\n",
    "        out = self.backbone(\n",
    "            x_seq, ctx=ctx, pos_map=pos_map_resized, y=t_emb,\n",
    "            shared_adaln=shared_adaln_state, return_enc_out=return_enc_out,\n",
    "            deterministic=deterministic\n",
    "        )\n",
    "        if return_enc_out:\n",
    "            out, enc_out = out\n",
    "\n",
    "        # TREAD (post)\n",
    "        if self.use_tread:\n",
    "            if (not deterministic) or (tread_rate is not None):\n",
    "                full_len = (raw_pos.shape[1])\n",
    "                def _scatter_back(x_row, sel_mask_row):\n",
    "                    idx = jnp.nonzero(sel_mask_row, size=sel_mask_row.shape[0], fill_value=0)[0]\n",
    "                    idx = idx[:x_row.shape[0]]\n",
    "                    base = jnp.zeros((full_len, x_row.shape[-1]), dtype=x_row.dtype)\n",
    "                    return base.at[idx].set(x_row)\n",
    "                out_full = jax.vmap(_scatter_back)(out, sel_mask)\n",
    "                out = out_full\n",
    "                pos_map_resized = raw_pos\n",
    "\n",
    "            out = self.post_tread_trns(\n",
    "                out, ctx=ctx, pos_map=pos_map_resized, y=t_emb,\n",
    "                shared_adaln=shared_adaln_state, deterministic=deterministic\n",
    "            )\n",
    "\n",
    "        out = out[:, :length]\n",
    "        img = self.out_patch(out, axis1=H, axis2=W)\n",
    "        return img if not return_enc_out else (img, enc_out[:, :length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c388a809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XUDiT output shape: (2, 4, 32, 32)\n",
      " Test passed  output shape matches input (NCHW).\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "B = 2          # batch size\n",
    "H = 32         # height\n",
    "W = 32         # width\n",
    "C = 4          # input channels (: latent image or feature map)\n",
    "D = 64         # transformer dim  \n",
    "ctx_dim = 32   # context embedding dim\n",
    "ctx_size = 8   # context length (for conditioning)\n",
    "\n",
    "x = jax.random.normal(key, (B, H, W, C))\n",
    "t = jnp.linspace(0, 1, B)  # timestep embedding input (B,)\n",
    "ctx = jax.random.normal(key, (B, ctx_size, ctx_dim))\n",
    "\n",
    "model = XUDiT(\n",
    "    patch_size=4,\n",
    "    input_dim=C,\n",
    "    dim=D,\n",
    "    ctx_dim=ctx_dim,\n",
    "    ctx_size=ctx_size,\n",
    "    heads=4,\n",
    "    dim_head=16,\n",
    "    mlp_dim=128,\n",
    "    depth=2,\n",
    "    enc_blocks=1,\n",
    "    dec_blocks=1,\n",
    "    class_cond=0,\n",
    "    shared_adaln=True,\n",
    "    concat_ctx=True,\n",
    "    use_dyt=False,\n",
    "    double_t=False,\n",
    "    addon_info_embs_dim=None,\n",
    "    tread_config=None,  # TREAD off\n",
    ")\n",
    "\n",
    "# forward pass\n",
    "variables = model.init(key, x, t, ctx)\n",
    "out = model.apply(variables, x, t, ctx)\n",
    "\n",
    "print(\" XUDiT output shape:\", out.shape)\n",
    "assert out.shape == (B, C, H, W)\n",
    "print(\" Test passed  output shape matches input (NCHW).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b81505ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.021492004394531\n",
      " Gradients finite: True\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "B, H, W, C = 2, 32, 32, 4\n",
    "D = 64\n",
    "ctx_dim, ctx_size = 32, 8\n",
    "\n",
    "x = jax.random.normal(key, (B, H, W, C))\n",
    "t = jnp.linspace(0, 1, B)\n",
    "ctx = jax.random.normal(key, (B, ctx_size, ctx_dim))\n",
    "\n",
    "model = XUDiT(\n",
    "    patch_size=4,\n",
    "    input_dim=C,\n",
    "    dim=D,\n",
    "    ctx_dim=ctx_dim,\n",
    "    ctx_size=ctx_size,\n",
    "    heads=4,\n",
    "    dim_head=16,\n",
    "    mlp_dim=128,\n",
    "    depth=2,\n",
    "    enc_blocks=1,\n",
    "    dec_blocks=1,\n",
    "    class_cond=0,\n",
    "    shared_adaln=True,\n",
    "    concat_ctx=True,\n",
    "    use_dyt=False,\n",
    "    double_t=False,\n",
    "    addon_info_embs_dim=None,\n",
    "    tread_config=None,\n",
    ")\n",
    "\n",
    "# Initialize parameters\n",
    "variables = model.init(key, x, t, ctx)\n",
    "\n",
    "# Define dummy loss (MSE)\n",
    "def loss_fn(params, x, t, ctx):\n",
    "    out = model.apply(params, x, t, ctx)\n",
    "    loss = jnp.mean((out - x.transpose(0, 3, 1, 2)) ** 2)  # match NCHW order\n",
    "    return loss\n",
    "\n",
    "# Compute loss and gradient\n",
    "loss_val, grads = jax.value_and_grad(loss_fn)(variables, x, t, ctx)\n",
    "\n",
    "#  Sanity check\n",
    "print(\"Loss:\", float(loss_val))\n",
    "# Check gradient finiteness\n",
    "def has_nan(x):\n",
    "    return jnp.isnan(x).any()\n",
    "\n",
    "nan_in_grad = jax.tree_util.tree_reduce(lambda a, b: a or has_nan(b), grads, False)\n",
    "print(\" Gradients finite:\", not bool(nan_in_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "852390c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 00 | Loss: 4.696184\n",
      "Step 01 | Loss: 3.102890\n",
      "Step 02 | Loss: 2.428725\n",
      "Step 03 | Loss: 2.197905\n",
      "Step 04 | Loss: 1.968389\n",
      "Step 05 | Loss: 1.933393\n",
      "Step 06 | Loss: 1.750799\n",
      "Step 07 | Loss: 1.765865\n",
      "Step 08 | Loss: 1.664528\n",
      "Step 09 | Loss: 1.563093\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "# ============================================================\n",
    "# 1.  \n",
    "# ============================================================\n",
    "B, H, W, C = 2, 16, 16, 4\n",
    "D = 32\n",
    "ctx_dim, ctx_size = 16, 4\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(key, (B, H, W, C))\n",
    "t = jnp.linspace(0, 1, B)\n",
    "ctx = jax.random.normal(key, (B, ctx_size, ctx_dim))\n",
    "\n",
    "model = XUDiT(\n",
    "    patch_size=4,\n",
    "    input_dim=C,\n",
    "    dim=D,\n",
    "    ctx_dim=ctx_dim,\n",
    "    ctx_size=ctx_size,\n",
    "    heads=2,\n",
    "    dim_head=8,\n",
    "    mlp_dim=64,\n",
    "    depth=1,\n",
    "    enc_blocks=1,\n",
    "    dec_blocks=1,\n",
    "    class_cond=0,\n",
    "    shared_adaln=True,\n",
    "    concat_ctx=True,\n",
    "    use_dyt=False,\n",
    "    double_t=False,\n",
    "    addon_info_embs_dim=None,\n",
    "    tread_config=None,\n",
    ")\n",
    "\n",
    "variables = model.init(key, x, t, ctx)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Optimizer + Train state\n",
    "# ============================================================\n",
    "tx = optax.adamw(learning_rate=1e-3)\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "state = TrainState.create(apply_fn=model.apply, params=variables['params'], tx=tx)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Diffusion-style loss \n",
    "# ============================================================\n",
    "def loss_fn(params, x, t, ctx, key):\n",
    "    # ground truth noise  ,  _pred  \n",
    "    noise = jax.random.normal(key, x.shape)\n",
    "    x_noisy = x + 0.1 * noise  #   \n",
    "    pred = model.apply({'params': params}, x_noisy, t, ctx)\n",
    "    loss = jnp.mean((pred - noise.transpose(0, 3, 1, 2)) ** 2)\n",
    "    return loss\n",
    "\n",
    "# ============================================================\n",
    "# 4.  step \n",
    "# ============================================================\n",
    "@jax.jit\n",
    "def train_step(state, x, t, ctx, key):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params, x, t, ctx, key)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# ============================================================\n",
    "# 5.   \n",
    "# ============================================================\n",
    "for step in range(10):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    state, loss = train_step(state, x, t, ctx, subkey)\n",
    "    if step % 1 == 0:\n",
    "        print(f\"Step {step:02d} | Loss: {float(loss):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0f3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_beta_schedule(T=1000, beta_start=1e-4, beta_end=2e-2):\n",
    "    #    (CPU )\n",
    "    betas = jnp.linspace(beta_start, beta_end, T, dtype=jnp.float32)\n",
    "    alphas = 1.0 - betas\n",
    "    alpha_bars = jnp.cumprod(alphas)\n",
    "    return betas, alphas, alpha_bars\n",
    "\n",
    "BETAS, ALPHAS, ALPHA_BARS = lin_beta_schedule()\n",
    "\n",
    "def loss_fn(params, x0, t_cont, ctx, key):\n",
    "    \"\"\"\n",
    "    x0: (B,H,W,C), t_cont: (B,)[0,1]\n",
    "      NCHW  transpose   \n",
    "    \"\"\"\n",
    "    B = x0.shape[0]\n",
    "    #  t discrete step \n",
    "    T = ALPHA_BARS.shape[0]\n",
    "    t_idx = jnp.clip((t_cont * (T - 1)).astype(jnp.int32), 0, T - 1)\n",
    "\n",
    "    alpha_bar_t = ALPHA_BARS[t_idx]                 # (B,)\n",
    "    sqrt_ab = jnp.sqrt(alpha_bar_t)[:, None, None, None]\n",
    "    sqrt_one_minus_ab = jnp.sqrt(1.0 - alpha_bar_t)[:, None, None, None]\n",
    "\n",
    "    noise = jax.random.normal(key, x0.shape)\n",
    "    x_t = sqrt_ab * x0 + sqrt_one_minus_ab * noise\n",
    "\n",
    "    #  (B,C,H,W)    noise  \n",
    "    eps_pred = model.apply({'params': params}, x_t, t_cont, ctx)     # (B,C,H,W)\n",
    "    eps_tgt = noise.transpose(0, 3, 1, 2)                            # (B,C,H,W)\n",
    "\n",
    "    return jnp.mean((eps_pred - eps_tgt) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f880904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3030723/1616844922.py:27: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree.leaves (jax v0.4.25 or newer) or jax.tree_util.tree_leaves (any JAX version).\n",
      "  grad_norm = jnp.sqrt(sum([jnp.sum(g**2) for g in jax.tree_leaves(grads)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 00 | Loss: 4.491674 | grad_norm: 14.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3030723/1616844922.py:16: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  return jax.tree_map(lambda e, p: decay * e + (1 - decay) * p, ema_params, new_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 01 | Loss: 3.430302 | grad_norm: 9.012\n",
      "Step 02 | Loss: 2.891752 | grad_norm: 5.750\n",
      "Step 03 | Loss: 2.492566 | grad_norm: 4.882\n",
      "Step 04 | Loss: 2.378393 | grad_norm: 3.864\n",
      "Step 05 | Loss: 2.092716 | grad_norm: 2.930\n",
      "Step 06 | Loss: 1.965346 | grad_norm: 2.324\n",
      "Step 07 | Loss: 1.986635 | grad_norm: 1.969\n",
      "Step 08 | Loss: 1.732124 | grad_norm: 1.480\n",
      "Step 09 | Loss: 1.677979 | grad_norm: 1.258\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "from flax.training import train_state\n",
    "from flax.core import FrozenDict\n",
    "\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),       #  \n",
    "    optax.adamw(1e-3, weight_decay=0.0),\n",
    ")\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    ema_params: FrozenDict | None = None\n",
    "\n",
    "def ema_update(ema_params, new_params, decay=0.999):\n",
    "    if ema_params is None:    #   bootstrap\n",
    "        return new_params\n",
    "    return jax.tree_map(lambda e, p: decay * e + (1 - decay) * p, ema_params, new_params)\n",
    "\n",
    "state = TrainState.create(apply_fn=model.apply, params=variables['params'], tx=tx, ema_params=None)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, x0, t_cont, ctx, key):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params, x0, t_cont, ctx, key)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    new_ema = ema_update(state.ema_params, state.params, decay=0.999)\n",
    "    state = state.replace(ema_params=new_ema)\n",
    "    # grad norm \n",
    "    grad_norm = jnp.sqrt(sum([jnp.sum(g**2) for g in jax.tree_leaves(grads)]))\n",
    "    return state, loss, grad_norm\n",
    "\n",
    "# \n",
    "for step in range(10):\n",
    "    key, sub = jax.random.split(key)\n",
    "    state, loss, gnorm = train_step(state, x, t, ctx, sub)\n",
    "    print(f\"Step {step:02d} | Loss: {float(loss):.6f} | grad_norm: {float(gnorm):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000] loss=2.65521 gnorm=4.101\n",
      "[005] loss=1.93928 gnorm=1.516\n",
      "[010] loss=1.61671 gnorm=0.787\n",
      "[015] loss=1.57689 gnorm=0.718\n",
      "[020] loss=1.36980 gnorm=0.648\n",
      "[025] loss=1.29856 gnorm=0.556\n",
      "[030] loss=1.12467 gnorm=0.532\n",
      "[035] loss=1.12139 gnorm=0.512\n",
      "[040] loss=1.10809 gnorm=0.443\n",
      "[045] loss=0.99940 gnorm=0.420\n"
     ]
    }
   ],
   "source": [
    "def sample_batch(key, B=8, H=16, W=16, C=4, ctx_size=4, ctx_dim=16):\n",
    "    k1,k2,k3 = jax.random.split(key, 3)\n",
    "    x0  = jax.random.normal(k1, (B,H,W,C))\n",
    "    t   = jax.random.uniform(k2, (B,), minval=0.0, maxval=1.0)\n",
    "    ctx = jax.random.normal(k3, (B, ctx_size, ctx_dim))\n",
    "    return x0, t, ctx\n",
    "\n",
    "for step in range(50):\n",
    "    key, sub = jax.random.split(key)\n",
    "    xb, tb, ctxb = sample_batch(sub)\n",
    "    key, sub2 = jax.random.split(key)\n",
    "    state, loss, gnorm = train_step(state, xb, tb, ctxb, sub2)\n",
    "    if step % 5 == 0:\n",
    "        print(f\"[{step:03d}] loss={float(loss):.5f} gnorm={float(gnorm):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955618f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_eps_pred(params, x_t, t_cont, ctx):\n",
    "    return model.apply({'params': params}, x_t, t_cont, ctx)\n",
    "\n",
    "def simple_denoise(key, params, ctx, H=16, W=16, C=4, steps=16):\n",
    "    B = ctx.shape[0]\n",
    "    x = jax.random.normal(key, (B,H,W,C))\n",
    "    for i in range(steps, 0, -1):\n",
    "        t_cont = jnp.full((B,), i/steps, dtype=jnp.float32)\n",
    "        eps = sample_eps_pred(params, x, t_cont, ctx)             # (B,C,H,W)\n",
    "        eps = eps.transpose(0,2,3,1)                              #  (B,H,W,C)\n",
    "        #    Euler ( scheduler )\n",
    "        x = x - 0.1 * eps\n",
    "    return x  #  /latent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc2bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " shape: (2, 16, 16, 4)\n",
      "  (/): 0.07229324 3.2548957\n"
     ]
    }
   ],
   "source": [
    "key, sub = jax.random.split(key)\n",
    "# ctx   shape \n",
    "ctx_sample = jax.random.normal(sub, (2, 4, 16))  # (B, ctx_size, ctx_dim)\n",
    "\n",
    "# EMA      ,  state.params \n",
    "params = state.ema_params if state.ema_params is not None else state.params\n",
    "\n",
    "# denoise \n",
    "key, sub2 = jax.random.split(key)\n",
    "sampled = simple_denoise(sub2, params, ctx_sample, H=16, W=16, C=4, steps=16)\n",
    "\n",
    "print(\" shape:\", sampled.shape)\n",
    "print(\"  (/):\", jnp.mean(sampled), jnp.std(sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0f90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
